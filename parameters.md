# Transformers-With-TensorFlow2 Parameters

## Attention

- `num_attention_heads`
- `size_per_head_for_query_and_key`
- `size_per_head_for_value`
- `attention_dropout_rate`
- `use_bias`
- `output_shape*`
- `attention_axes`
- `return_attention_scores`
- `norm_first`

## Position-Wise Feed-Forward Network

- `intermediate_size`
- `intermediate_activation`
- `hidden_dropout_rate`

## Common

- `kernel_initializer`
- `bias_initializer`
- `kernel_regularizer`
- `bias_regularizer`
- `kernel_constraint`
- `bias_constraint`
- `activity_regularizer`

## Other

- `hidden_dropout_rate`

## Bert Config

- `attention_probs_dropout_prob`
- `hidden_act`
- `hidden_dropout_prob`
- `hidden_size`
- `initializer_range`
- `intermediate_size`
- `max_position_embeddings`
- `num_attention_heads`
- `num_hidden_layers`
- `type_vocab_size`
- `vocab_size`

